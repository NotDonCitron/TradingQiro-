#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Debug-Skript um das Scraping-Problem zu analysieren
Testet einen echten Cryptet-Link um zu sehen was gescraped wird
"""

import asyncio
import os
import sys
from datetime import datetime
import requests
from dotenv import load_dotenv

# Lade Umgebungsvariablen
load_dotenv()

class ScrapingDebugger:
    """Debug-Klasse fÃ¼r Scraping-Probleme"""
    
    def __init__(self):
        self.bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "8496816723:AAHH-YVZdmoueA_cV9lcJncUyIR3N3Vizbw")
        self.target_group_id = int(os.getenv("OWN_GROUP_CHAT_ID", "-1002773853382"))
        
        # URLs aus dem letzten Test
        self.test_urls = [
            "https://cryptet.com/signals/one/link_usdt/2025/08/24/1517?utm_campaign=notification&utm_medium=telegram",
            "https://cryptet.com/signals/one/xrp_usdt/2025/08/24/1456?utm_campaign=notification&utm_medium=telegram",
            "https://cryptet.com/signals/one/trx_usdt/2025/08/24/1440?utm_campaign=notification&utm_medium=telegram",
            "https://cryptet.com/signals/one/ltc_usdt/2025/08/24/1415?utm_campaign=notification&utm_medium=telegram"
        ]
    
    async def send_telegram_message(self, chat_id: str, message: str) -> bool:
        """Sende Nachricht Ã¼ber Telegram Bot API"""
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
            
            data = {
                "chat_id": chat_id,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": True
            }
            
            response = requests.post(url, json=data, timeout=10)
            return response.status_code == 200
                
        except Exception as e:
            print(f"âŒ Telegram Send Error: {e}")
            return False
    
    async def debug_single_url(self, url: str, url_number: int):
        """Debug eine einzelne URL um zu sehen was gescraped wird"""
        try:
            from src.connectors.cryptet_scraper import CryptetScraper
            
            print(f"\nğŸ” DEBUGGING URL {url_number}: {url}")
            print("-" * 80)
            
            # Initialisiere Scraper
            scraper = CryptetScraper(headless=False)  # Nicht headless fÃ¼r bessere Debugging
            
            # Initialisiere Browser
            success = await scraper.initialize_browser()
            if not success:
                print(f"âŒ Browser konnte nicht initialisiert werden")
                return None
            
            print(f"âœ… Browser initialisiert")
            
            # Scrape das Signal
            start_time = asyncio.get_event_loop().time()
            
            signal_data = await scraper.scrape_signal(url)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            print(f"ğŸ• Scraping-Zeit: {processing_time:.2f}s")
            
            if signal_data:
                print(f"âœ… Signal erfolgreich gescraped!")
                print(f"ğŸ“Š VollstÃ¤ndige Signal-Daten:")
                for key, value in signal_data.items():
                    print(f"   {key}: {value}")
                
                # Formatiere das Signal fÃ¼r Cornix
                from src.core.cryptet_automation import CryptetAutomation
                automation = CryptetAutomation()
                formatted_signal = automation._format_cornix_signal(signal_data)
                
                if formatted_signal:
                    print(f"\nğŸ“¤ Formatiertes Signal fÃ¼r Cornix:")
                    print(formatted_signal)
                    
                    # Sende das formatierte Signal
                    debug_msg = f"""ğŸ” **DEBUG SCRAPING URL {url_number}** ğŸ”

âœ… **Scraping erfolgreich:** {processing_time:.2f}s
ğŸ”— **URL:** {url[:60]}...

ğŸ“Š **Rohdaten:**
â€¢ Symbol: {signal_data.get('symbol', 'N/A')}
â€¢ Direction: {signal_data.get('direction', 'N/A')}
â€¢ Entry: {signal_data.get('entry_price', 'N/A')}
â€¢ Stop Loss: {signal_data.get('stop_loss', 'N/A')}
â€¢ Take Profits: {signal_data.get('take_profits', 'N/A')}

ğŸ“¤ **Formatiertes Signal:**
```
{formatted_signal}
```"""
                    
                    await self.send_telegram_message(str(self.target_group_id), debug_msg)
                else:
                    print(f"âŒ Fehler beim Formatieren des Signals")
                    error_msg = f"""âš ï¸ **DEBUG URL {url_number} - FORMATIERUNGSFEHLER** âš ï¸

âœ… **Scraping:** Erfolgreich ({processing_time:.2f}s)
âŒ **Formatierung:** Fehlgeschlagen

ğŸ“Š **Rohdaten waren:** {signal_data}
ğŸ”§ **Problem:** _format_cornix_signal() returnierte None"""
                    
                    await self.send_telegram_message(str(self.target_group_id), error_msg)
            else:
                print(f"âŒ Kein Signal gescraped")
                error_msg = f"""âŒ **DEBUG URL {url_number} - SCRAPING FEHLGESCHLAGEN** âŒ

ğŸ• **Zeit:** {processing_time:.2f}s
ğŸ”— **URL:** {url[:60]}...

âš ï¸ **Problem:** scrape_signal() returnierte None
ğŸ”§ **MÃ¶gliche Ursachen:**
â€¢ Seite nicht geladen
â€¢ Struktur der Seite geÃ¤ndert
â€¢ Extraction-Pattern passen nicht
â€¢ Browser-Probleme"""
                
                await self.send_telegram_message(str(self.target_group_id), error_msg)
            
            # Cleanup
            await scraper.close()
            
            return signal_data
            
        except Exception as e:
            print(f"âŒ Fehler beim Debugging: {e}")
            import traceback
            traceback.print_exc()
            
            error_msg = f"""âŒ **DEBUG URL {url_number} - EXCEPTION** âŒ

âš ï¸ **Fehler:** {str(e)[:100]}...
ğŸ”§ **Details:** Siehe Logs fÃ¼r vollstÃ¤ndigen Stacktrace"""
            
            await self.send_telegram_message(str(self.target_group_id), error_msg)
            return None
    
    async def run_complete_debug(self):
        """FÃ¼hre komplettes Debug aller URLs durch"""
        
        print("ğŸ” DEBUGGING CRYPTET SCRAPING PROBLEM")
        print("=" * 60)
        print("Testet echte Cryptet URLs um herauszufinden warum kein Signal gescraped wird")
        print()
        
        # Sende Start-Nachricht
        start_msg = f"""ğŸ” **CRYPTET SCRAPING DEBUG GESTARTET** ğŸ”

ğŸ• **Start:** {datetime.now().strftime('%H:%M:%S')}
ğŸ¯ **Ziel:** Herausfinden warum Signale nicht gescraped werden

ğŸ“‹ **Test-URLs:** {len(self.test_urls)} echte Cryptet-Links
ğŸ”§ **Methode:** Einzelverarbeitung mit detaillierter Analyse

ğŸ”„ **Status:** Debug beginnt..."""
        
        await self.send_telegram_message(str(self.target_group_id), start_msg)
        
        results = []
        
        # Debug jede URL einzeln
        for i, url in enumerate(self.test_urls, 1):
            result = await self.debug_single_url(url, i)
            results.append({
                'url_number': i,
                'url': url,
                'success': result is not None,
                'data': result
            })
            
            # Pause zwischen URLs
            if i < len(self.test_urls):
                print(f"â³ Pause zwischen URLs...")
                await asyncio.sleep(3)
        
        # Zusammenfassung
        successful = sum(1 for r in results if r['success'])
        
        summary_msg = f"""ğŸ“Š **CRYPTET SCRAPING DEBUG ABGESCHLOSSEN** ğŸ“Š

ğŸ• **Ende:** {datetime.now().strftime('%H:%M:%S')}
ğŸ“ˆ **Erfolgsrate:** {successful}/{len(results)} URLs

âœ… **Erfolgreich:** {successful}
âŒ **Fehlgeschlagen:** {len(results) - successful}

ğŸ“‹ **Detaillierte Ergebnisse:**"""
        
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            url_short = result['url'].split('/')[-1][:20]
            summary_msg += f"\n{status} URL {result['url_number']}: {url_short}..."
        
        if successful == 0:
            summary_msg += f"\n\nâŒ **PROBLEM IDENTIFIZIERT:**\nKEINE der URLs konnte erfolgreich gescraped werden!"
            summary_msg += f"\n\nğŸ”§ **MÃ–GLICHE URSACHEN:**\nâ€¢ Cryptet Website-Struktur geÃ¤ndert\nâ€¢ Browser/Selenium-Problem\nâ€¢ Cookie/Authentication-Problem\nâ€¢ Extraction-Pattern veraltet"
        elif successful < len(results):
            summary_msg += f"\n\nâš ï¸ **TEILPROBLEM:**\nNur {successful}/{len(results)} URLs funktionieren"
        else:
            summary_msg += f"\n\nğŸ‰ **ALLES FUNKTIONIERT:**\nAlle URLs wurden erfolgreich gescraped!"
        
        await self.send_telegram_message(str(self.target_group_id), summary_msg)
        
        return successful, len(results)

async def main():
    """Hauptfunktion"""
    
    # ÃœberprÃ¼fe Systemumgebung
    if not os.path.exists("src"):
        print("âŒ Fehler: 'src' Verzeichnis nicht gefunden")
        print("Bitte aus dem Hauptprojektverzeichnis ausfÃ¼hren")
        sys.exit(1)
    
    # FÃ¼ge src zum Python Path hinzu
    if "src" not in sys.path:
        sys.path.insert(0, "src")
        sys.path.insert(0, ".")
    
    debugger = ScrapingDebugger()
    
    try:
        successful, total = await debugger.run_complete_debug()
        
        if successful == total:
            print("\nâœ… SCRAPING FUNKTIONIERT KORREKT!")
            print("Das Problem liegt woanders im System.")
        elif successful > 0:
            print(f"\nâš ï¸ TEILWEISE FUNKTIONSFÃ„HIG: {successful}/{total}")
            print("Einige URLs funktionieren, andere nicht.")
        else:
            print(f"\nâŒ SCRAPING KOMPLETT DEFEKT!")
            print("Keine der URLs konnte gescraped werden.")
            
    except Exception as e:
        print(f"âŒ Debug fehlgeschlagen: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ” CRYPTET SCRAPING DEBUGGER")
    print("Analysiert warum keine Signale gescraped werden")
    print()
    
    asyncio.run(main())